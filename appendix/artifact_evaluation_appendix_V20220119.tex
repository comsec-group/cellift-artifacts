% LaTeX template for Artifact Evaluation V20220119
%
% Original Authors 
% * Grigori Fursin (cTuning foundation, France) 2014-2020
% * Bruce Childers (University of Pittsburgh, USA) 2014
% 
% Modified by
% * Cl√©mentine Maurice (CNRS, France) 2021-2022
% * Cristiano Giuffrida (Vrije Universiteit Amsterdam, Netherlands) 2021-2022
%
% (C)opyright 2014-2022
%
% CC BY 4.0 license
%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}

\usepackage[hyphens]{xurl}                % allow \url in bibtex for clickable links

% BJG - to suppress a warning
\microtypecontext{spacing=nonfrench}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{xspace}

% inlined bib file
% BJG -commented out to avoid the warning, seems harmless
%\usepackage{filecontents}

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{comment}
\usepackage{relsize}
\usepackage[inline]{enumitem}
\usepackage{multirow}

% colored Observation boxes
\usepackage{tcolorbox}

\usepackage{hyperref}

\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To add the badges to the final version after notification, first download
% the appropriate `usenixbadges` package version as detailed on the Final
% Artifact Appendix instructions page on the USENIX Security website.
%
% Then, uncomment the following line using the appropriate options:
% \usepackage[available,functional,reproduced]{usenixbadges}
%
% In the options, list the badges that have been awarded to your paper.
% The possible badges are:
%
%  * `available`  --- affix the "Artifacts Available" badge
%  * `functional` --- affix the "Artifacts Functional" badge
%  * `reproduced` --- affix the "Results Reproduced" badge
%
% Example:
%  \usepackage[available,functional]{usenixbadges}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\special{papersize=8.5in,11in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove the part above
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\ourname}{C{\smaller[1]ELL}IFT\xspace}

\appendix
\section{Artifact Appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% See the Artifact Appendix guidelines page on the USENIX
% Security website to compile the appendix. Please preserve
% the provided Artifact Appendix template as much as
% possible (e.g., keep the original (sub)section names
% and order). 
%
% See also examples of past reproduced papers with a similar Artifact Appendix at: https://cknowledge.io/?q=%22reproduced-papers%22+AND+lib+AND+%28secur*+OR+harden*+OR+mitigat*+OR+defen*+OR+attack*+OR+bug*+OR+vulnerab*%29
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

In our Artifact, we provide full source code of \ourname, and source code
of a native toolchain and RISCV toolchain and other dependencies. We also
provide source code to perform all experiments described in this paper,
and to analyze the results. All of this is packaged as a Docker image to
allow optimal reproducibility. To reproduce all experiments, we expect that
a machine with 256GB memory and 500GB of free storage will be needed. Less memory is needed if the
precompiled executables of the designs are used so that synthesis (i.e.\ compiling)
isn't necessary. In that case we believe 64GB will be enough.

\subsection{Artifact check-list}

% {\em Obligatory. Fill in whatever is applicable with some keywords and remove unrelated items.}

{\small
\begin{itemize}
  \item {\bf Algorithm: } In the Yosys pass that implements \ourname, a newly developed algorithm to efficiently generate IFT shadow logic is presented.
  \item {\bf Program: } We use a set of 5 external Verilog designs (Ariane, BOOM, Ibex, Rocket, PULPissimo) as evaluation targets, as well as benchmarks
      from the RISC-V Architectural testing framework. All of this code is included in our artifact.
  \item {\bf Compilation: } We include the required compilers and interpreters.
  \item {\bf Transformations: } We include the required Verilog transformations (\ourname and GLIFT), implemented as Yosys passes.
  \item {\bf Binary: } We include prebuilt Verilator-built binaries of all designs (5, see above) in all instrumentation modes (none (also known as Vanilla), CellIFT, GLIFT) where possible (sometimes GLIFT fails, as explained in this paper).
  \item {\bf Run-time environment: } The bulk of our artifact is a Docker image assuming a Linux kernel. We tested our image on an Ubuntu 22.04 system with 5.15.0-37-generic kernel.
  \item {\bf Hardware: } We do not require any special hardware, but do need a relatively large amount of RAM (256GB) to run all experiments.
  \item {\bf Metrics: } The experiments record runtime performance and IFT precision for microbenchmarks for \ourname as well as GLIFT. Further experiments record execution time and memory footprint of the instrumentation and synthesis process for uninstrumented designs, as well as \ourname and GLIFT instrumentations. We also measure benchmark simulation performance on instrumented designs (\ourname and GLIFT) vs uninstrumented designs. Lastly, we show resource usage and clockable frequency after FPGA synthesis for all 5 designs each under all 3 instrumentation modes, where possible.
  \item {\bf Output: } For all experiments used in the Evaluation section of this paper, we include code to regenerate the charts. Also, we include code to reproduce all results in the Scenarios section of this paper: the Meltdown and Spectre scenarios, and the architectural vulnerabilities. 
  \item {\bf Experiments: } With the exception of the FPGA results, all experiments were executed automatically when the Docker image was built. This means the way to reproduce all experiments is encoded in the Dockerfile, and a Docker container can be run that contains the generated results, and can be used to re-run individual experiments if desired. 
  \item {\bf How much disk space required: } The docker image with all layers is 330GB, Xilinx Vivado is a large download (around 70GB) and install (around 80GB), so we estimate a total of 500GB free storage will be used.
  \item {\bf How much time is needed to prepare workflow: } To prepare the workflow, conscious effort is only needed to retrieve the Git repository and the Docker image, a total amount of effort of only 5 minutes. For these steps to finish will no doubt take many hours, depending on available Internet bandwidth.
  \item {\bf How much time is needed to complete experiments: } Approximately 3 days. 
  \item {\bf Publicly available: } Stable URL: \url{https://github.com/comsec-group/cellift-artifacts/commit/372b50df8bd91ad49295a6cf39f9b98359c107d4}. The README points to a stable (sha256-verified) Docker image that contains the rest of the code and data.
  \item {\bf Code licenses: } \ourname is licensed under GPL3. 
  \item {\bf Workflow frameworks used: } Docker, Make, Luigi. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

% {\em Obligatory. For inapplicable subsections (e.g., the ``How to access'' subsection when not applying for the ``Artifacts Available'' badge), please specify 'N/A'.}

\subsubsection{How to access}

The evolving project is hosted at the URL mentioned in the Introduction
of this paper. Our artifact is a single Git repository designed primarily to
build a Docker image that has run all the experiments automatically. This
Git repository is hosted at the `Publicly available' checklist entry above.
The README.md in that repository contains further instructions to obtain the
prebuilt Docker image from Dockerhub.

\subsubsection{Hardware dependencies}

The artifact will run all experiments on a machine with 256GB of memory. A subset can be run using the prebuilt binaries on a 64GB machine.

\subsubsection{Software dependencies}

As it'sa Docker container, we expect the artifact to run on a wide range
of Linux distributions, however we tested it on Ubuntu 22.04 LTS kernel
5.15.0-37-generic.

To reproduce the FPGA experiments in the paper, we furthermore depend
on the Xilinx Vivado FPGA synthesis tool (version 2019.3).  The large
FPGA target we use requires a Vivado license. Vivado needs to be downloaded
and installed from its manufacturer's website.

\subsubsection{Data sets}

N/A

\subsubsection{Models}

N/A

\subsubsection{Security, privacy, and ethical concerns}

N/A

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}

At a high level, installation of our artifact is done by
\begin{enumerate}
    \item Cloning the git repository specified in the checklist.
    \item Using its README.md, pull the Docker image artifact
        hosted on Dockerhub.
    \item To reproduce the FPGA experiments: following instructions
        from the Xilinx website, install the full edition of Vivado.
        This requires a license. To use it from the \ourname
        FPGA evaluation dir, source the settings64.sh
        file.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment workflow}

Follow the instructions in the git repository README.md that specifies
in detail how to start a Docker container with the image, and how to
reproduce each experiment, and examine the results.

In principle, cloning the git repository and rebuilding the Docker image
using the Dockerfile in the git repository will rebuild all \ourname code
and designs from scratch and perform the experiments, but for maximum
reliability we also provide the Docker image with all code, binaries
and results that we have found to work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and expected results}

% {\em Obligatory. Start by listing the main claims in your paper. Next, list your key results and detail how they each support the main claims. Finally, detail all the steps to reproduce each of the key results in your paper by running the artifacts. Describe the expected results and the maximum variation of empirical results (particularly important for performance numbers).}

The main claims in our paper are the following.

\begin{enumerate}
    \item \ourname is an RTL dynamic information flow tracking instrumentation framework that scales up to complex CPU and SoC designs.
    \item \ourname is able to instrument larger designs than the competition (GLIFT), consumes less time and memory while instrumenting, and when the resulting instrumented designs are simulated, has a lower performance overhead than the competition (GLIFT), and with a higher tainting precision.
    \item \ourname is able to detect microarchitectural design flaws such as Meltdown and Spectre.
    \item \ourname is able to detect architectural design flaws such as missing reset functionality.
    \item \ourname-instrumented designs are synthesizable, and can be synthesized with typically less resources and to a typically higher frequency than the competition (GLIFT).
\end{enumerate}

The key results from our experiments are as follows. For each result, we point to scripts (Python or bash) that drive the experiments and show the analysis.
\begin{enumerate}
    \item Instrumented designs that can synthesize to C++ (i.e. be compiled) for all 5 target designs, contrary to the competition, and with fewer cpu time and less memory (follows from plot\_instrumentation\_performance.py and plot\_rss.py), and with higher tainting precision (follows from plot\_num\_tainted\_states\_ibex.py).
    \item For the designs that can be compiled in both instrumentation modes, we show a lower performance overhead than GLIFT (follows from plot\_benchmark\_performance.py). 
    \item The Meltdown and Spectre simulations reproduce Figure 11, showing they can both be detected (follows from plot\_tainted\_elements.py).
    \item We show several bug scenarios detected by \ourname (run\_scenarios.sh).
    \item We show FPGA synthesis results, showing that \ourname instrumented designs can be synthesized, with fewer resources than GLIFT instrumented designs \textbf{TODO Do we have a script?}.
\end{enumerate}

As there are many details, and long lines to copy and paste, we'd like
to refer to the README.md of the artifact git repository for the detailed
steps to reproduce each of the key results described above.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment customization}

There is ample customization opportunity in the Docker image, because
the code of the instrumentation tool as well as the target designs are
there and can be modified and rebuilt. This does require a deeper knowledge
than we can describe in this appendix. We have not prepared any
specific helper material for customization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Version}
%%%%%%%%%%%%%%%%%%%%
% Obligatory.
% Do not change/remove.
%%%%%%%%%%%%%%%%%%%%
Based on the LaTeX template for Artifact Evaluation V20220119.

\end{document}
